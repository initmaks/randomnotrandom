{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The story of neural net layers that traveled back in time to the point when they weren't trained.\n",
    "\n",
    "----\n",
    "\n",
    "Experiment setups:\n",
    "\n",
    "0. Randomly initialize a FC neural network and save a copy of it.\n",
    "1. Train until a good performance is achieved.\n",
    "2. Load:\n",
    "\n",
    "    a. random network and replace weights of one of it's layers with the trained ones.\n",
    "    \n",
    "    b. random network and replace weights of one of trained network layers with the intial weights.\n",
    "3. Get ready to have your mind blown! Once ready, test the network see the accuracy results.\n",
    "\n",
    "\n",
    "Credits: https://lilianweng.github.io/lil-log/2019/03/14/are-deep-neural-networks-dramatically-overfitted.html#experiments and The lottery ticket hypothesis paper.\n",
    "\n",
    "Training code is adapted from: https://docs.wandb.com/docs/frameworks/pytorch-example.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import copy\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FC_Net, self).__init__()\n",
    "#         fc_sizes = [1024,512,256,128,64]\n",
    "        fc_sizes = [2048,1024,512,256,128]\n",
    "        self.fc1 = nn.Linear(1*28*28, fc_sizes[0])\n",
    "        self.fc2 = nn.Linear(fc_sizes[0], fc_sizes[1])\n",
    "        self.fc3 = nn.Linear(fc_sizes[1], fc_sizes[2])\n",
    "        self.fc4 = nn.Linear(fc_sizes[2], fc_sizes[3])\n",
    "        self.fc5 = nn.Linear(fc_sizes[3], fc_sizes[4])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1*28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    example_images = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            # Save the first inbput tensor in each test batch as an example image\n",
    "            example_images.append(wandb.Image(data[0], caption=\"Pred: {} Truth: {}\".format(pred[0].item(), target[0])))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    # Log the images and metrics\n",
    "    wandb.log({\n",
    "            \"Examples\": example_images,\n",
    "            \"Test Accuracy\": 100. * correct / len(test_loader.dataset),\n",
    "            \"Test Loss\": test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/maks/generalisation/runs/9tda6m2a\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project='generalisation')\n",
    "# We load all of the arguments into config to save as hyperparameters\n",
    "# wandb.config.update(args)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=256, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=256, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FC_Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/anaconda3/envs/bup/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type FC_Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model,'init_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7f6e7b836be0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This magic line lets us save ther pytorch model and track all of the gradients and optionally parameters\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 4.850808\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 4.018377\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.199448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.1434, Accuracy: 3649/10000 (36%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.149435\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.690077\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.048092\n",
      "\n",
      "Test set: Average loss: 0.8945, Accuracy: 7702/10000 (77%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.921050\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.714349\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.575931\n",
      "\n",
      "Test set: Average loss: 0.5519, Accuracy: 8246/10000 (82%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.653642\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.490619\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.422941\n",
      "\n",
      "Test set: Average loss: 0.4028, Accuracy: 8850/10000 (88%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.405873\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.465752\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.359762\n",
      "\n",
      "Test set: Average loss: 0.3566, Accuracy: 8902/10000 (89%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.365309\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.289694\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.279088\n",
      "\n",
      "Test set: Average loss: 0.3234, Accuracy: 9042/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.298105\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.337713\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.237293\n",
      "\n",
      "Test set: Average loss: 0.2932, Accuracy: 9126/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.255464\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.293987\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.266238\n",
      "\n",
      "Test set: Average loss: 0.2763, Accuracy: 9192/10000 (92%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.235929\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.316476\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.241360\n",
      "\n",
      "Test set: Average loss: 0.2555, Accuracy: 9227/10000 (92%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.208428\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.265937\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.151979\n",
      "\n",
      "Test set: Average loss: 0.2464, Accuracy: 9264/10000 (93%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.223853\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.253326\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.254035\n",
      "\n",
      "Test set: Average loss: 0.2220, Accuracy: 9352/10000 (94%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.135288\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.200382\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.164513\n",
      "\n",
      "Test set: Average loss: 0.2118, Accuracy: 9379/10000 (94%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.269509\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.182058\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.199580\n",
      "\n",
      "Test set: Average loss: 0.1957, Accuracy: 9441/10000 (94%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.308965\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.117916\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.239632\n",
      "\n",
      "Test set: Average loss: 0.1903, Accuracy: 9471/10000 (95%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.190398\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.178484\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.176407\n",
      "\n",
      "Test set: Average loss: 0.1730, Accuracy: 9496/10000 (95%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.155053\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.210300\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.140387\n",
      "\n",
      "Test set: Average loss: 0.1693, Accuracy: 9494/10000 (95%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.186464\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.134066\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.096394\n",
      "\n",
      "Test set: Average loss: 0.1581, Accuracy: 9531/10000 (95%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.222658\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.180601\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.118772\n",
      "\n",
      "Test set: Average loss: 0.1509, Accuracy: 9549/10000 (95%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.134891\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.111701\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.132219\n",
      "\n",
      "Test set: Average loss: 0.1425, Accuracy: 9584/10000 (96%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.196860\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.171992\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.140591\n",
      "\n",
      "Test set: Average loss: 0.1436, Accuracy: 9582/10000 (96%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.186516\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.174695\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.088880\n",
      "\n",
      "Test set: Average loss: 0.1322, Accuracy: 9618/10000 (96%)\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.139637\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.098474\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.141008\n",
      "\n",
      "Test set: Average loss: 0.1267, Accuracy: 9613/10000 (96%)\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.153547\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.133984\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.066228\n",
      "\n",
      "Test set: Average loss: 0.1233, Accuracy: 9623/10000 (96%)\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.130105\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.090720\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.112691\n",
      "\n",
      "Test set: Average loss: 0.1263, Accuracy: 9618/10000 (96%)\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.110006\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.063543\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.131802\n",
      "\n",
      "Test set: Average loss: 0.1147, Accuracy: 9656/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/anaconda3/envs/bup/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type FC_Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "#Checkpoint\n",
    "torch.save(model,'trained_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading initial and trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model = torch.load('init_model.pt')\n",
    "trained_model = torch.load('trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0189, -0.0237,  0.0143, -0.0071,  0.0112,  0.0074, -0.0261,  0.0316,\n",
       "         0.0152, -0.0341,  0.0211, -0.0329,  0.0156,  0.0017,  0.0337,  0.0040,\n",
       "        -0.0022, -0.0355,  0.0055, -0.0091], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_model.fc1.weight[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0197, -0.0244,  0.0136, -0.0079,  0.0105,  0.0067, -0.0269,  0.0309,\n",
       "         0.0145, -0.0348,  0.0204, -0.0336,  0.0149,  0.0010,  0.0330,  0.0033,\n",
       "        -0.0029, -0.0362,  0.0048, -0.0098], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.fc1.weight[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different but very close, lets look into deeper layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0411,  0.0225, -0.0391, -0.0073,  0.0105,  0.0398,  0.0214,  0.0348,\n",
       "        -0.0152, -0.0315, -0.0428, -0.0353,  0.0102, -0.0082,  0.0332,  0.0086,\n",
       "         0.0048,  0.0239,  0.0248, -0.0066], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_model.fc4.weight[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0368,  0.0205, -0.0280, -0.0112,  0.0098,  0.0497,  0.0182,  0.0475,\n",
       "        -0.0152, -0.0353, -0.0462, -0.0329,  0.0099, -0.0082,  0.0324,  0.0087,\n",
       "         0.0050,  0.0239,  0.0415, -0.0066], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.fc4.weight[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still mostly within ±1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the accuracies on the test set for both of the models before changing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 4.8498, Accuracy: 0/10000 (0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(init_model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1147, Accuracy: 9656/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(trained_model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing fc1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3071, Accuracy: 9194/10000 (92%)\n",
      "\n",
      "replacing fc2\n",
      "\n",
      "Test set: Average loss: 0.8474, Accuracy: 9412/10000 (94%)\n",
      "\n",
      "replacing fc3\n",
      "\n",
      "Test set: Average loss: 0.5871, Accuracy: 9601/10000 (96%)\n",
      "\n",
      "replacing fc4\n",
      "\n",
      "Test set: Average loss: 0.6045, Accuracy: 9419/10000 (94%)\n",
      "\n",
      "replacing fc5\n",
      "\n",
      "Test set: Average loss: 1.1906, Accuracy: 8819/10000 (88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace weights of trained model with intial weights\n",
    "for fc_id in range(1,6):\n",
    "    new_model = copy.deepcopy(trained_model)\n",
    "    new_model._modules[f'fc{fc_id}'].weight = init_model._modules[f'fc{fc_id}'].weight\n",
    "    new_model._modules[f'fc{fc_id}'].bias = init_model._modules[f'fc{fc_id}'].bias\n",
    "    print(f'replacing fc{fc_id}')\n",
    "    test(new_model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing fc1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 4.8028, Accuracy: 29/10000 (0%)\n",
      "\n",
      "replacing fc2\n",
      "\n",
      "Test set: Average loss: 4.6531, Accuracy: 3743/10000 (37%)\n",
      "\n",
      "replacing fc3\n",
      "\n",
      "Test set: Average loss: 4.7018, Accuracy: 3113/10000 (31%)\n",
      "\n",
      "replacing fc4\n",
      "\n",
      "Test set: Average loss: 4.7312, Accuracy: 2709/10000 (27%)\n",
      "\n",
      "replacing fc5\n",
      "\n",
      "Test set: Average loss: 4.6319, Accuracy: 1536/10000 (15%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace weights of intial model with the trained weights\n",
    "for fc_id in range(1,6):\n",
    "    new_model = copy.deepcopy(init_model)\n",
    "    new_model._modules[f'fc{fc_id}'].weight = trained_model._modules[f'fc{fc_id}'].weight\n",
    "    new_model._modules[f'fc{fc_id}'].bias = trained_model._modules[f'fc{fc_id}'].bias\n",
    "    print(f'replacing fc{fc_id}')\n",
    "    test(new_model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single trained and cropped out layer placed in the randomly initialized network produces ~30-40% accuracy! \n",
    "# 🤯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - (bup)",
   "language": "python",
   "name": "bup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
